# -*- coding: utf-8 -*-
"""Capstone Modul 2 - Zayyan Rafishafa Kabullah Nugraha.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11y06_lhM-zH6XY_s6EFZNYSJNWTfO86s

#SaaS Sales Data Python Analysis - Zayyan Rafishafa Kabullah Nugraha

# Introduction

## Background

Amazon Web Services (AWS) SaaS is a company under Amazon that delivers a wide range of products and solutions, especially in the field of cloud computing. It supports three main service models: Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). Originally the data comes from [Amazon AWS Saas Sales Dataset Kaggle](https://www.kaggle.com/datasets/nnthanh101/aws-saas-sales) and currently the company is looking for a strategy to improve their sales and profits.

## Problem

The company is seeking to improve its product marketing effectiveness by identifying which **products** obtain the highest sales and which ones underperform in the market. The company also questions **strategies** according to **time trends** would be the best to improve their profits. The company currently needed a deep analysis of the product performance across different customer segments, industries, and regions. The data is expected to help the company to find the best strategy to market the underperforming products and help boost the the best selling products.

## Objective/Goals

- Find relevance between profit and sales with time trends
- Which products is profitable to push it's potential in the market
- What strategies can we use to increase the company's profits

Data Dictionary
"""

import pandas as pd

# Load the CSV
df = pd.read_csv('SaaS-Sales.csv')

# Create data dictionary
data_dictionary = {
    "Row ID": "A unique identifier for each transaction.",
    "Order ID": "A unique identifier for each order.",
    "Order Date": "The date when the order was placed.",
    "Date Key": "A numerical representation of the order date (YYYYMMDD).",
    "Contact Name": "The name of the person who placed the order.",
    "Country": "The country where the order was placed.",
    "City": "The city where the order was placed.",
    "Region": "The region where the order was placed.",
    "Subregion": "The subregion where the order was placed.",
    "Customer": "The name of the company that placed the order.",
    "Customer ID": "A unique identifier for each customer.",
    "Industry": "The industry the customer belongs to.",
    "Segment": "The customer segment (SMB, Strategic, Enterprise, etc.).",
    "Product": "The product that was ordered.",
    "License": "The license key for the product.",
    "Sales": "The total sales amount for the transaction.",
    "Quantity": "The total number of items in the transaction.",
    "Discount": "The discount applied to the transaction.",
    "Profit": "The profit from the transaction."
}

# Create DataFrame
dictionary_df = pd.DataFrame(list(data_dictionary.items()), columns=["Column Name", "Description"])

# Apply dark style using Styler
def dark_style(styler):
    return styler.set_table_styles([
        {'selector': 'thead th', 'props': [('background-color', '#222'), ('color', 'white'), ('font-weight', 'bold')]},
        {'selector': 'tbody td', 'props': [('background-color', '#333'), ('color', 'white')]},
        {'selector': 'tbody tr:hover td', 'props': [('background-color', '#444')]}
    ])

styled_df = dark_style(dictionary_df.style)
styled_df

"""This data dictionary shows the columns or names of each columns within the data and tells what they are. There are different information regarding different parts of the SaaS performance.

## Data Loading
"""

##Notes:
  #Some profits are more than sales, harusnya profit < sales : FIX PROFIT MARGIN / Profit Margin = Profit / Sales = Less than 1 and wouldn't go over -1 (0.xx)
  # Yang non decimals di drop

import pandas as pd # For data manipulation and analysis
import numpy as np # For numerical calculations
import matplotlib.pyplot as plt # For data visualization
import seaborn as sns # For more interesting data visualization
import missingno as msno # For identifying and visualizing missing data
from datetime import datetime # For date manipulation
from scipy import stats
from statsmodels.stats.diagnostic import lilliefors

data = pd.read_csv('SaaS-Sales.csv', sep=',')
print(data.head())

"""## Check Missing Values"""

import pandas as pd

# Load the CSV file
df = pd.read_csv('SaaS-Sales.csv')

#  Clean column names: remove leading/trailing spaces, lowercase, replace spaces with underscores
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")

# Display total number of missing values per column
print("Missing values per column:")
print(df.isnull().sum())

"""Clean Column Names

Cleaning column names of the data
"""

# Initial Exploration
print(df.head())        # preview first rows
print(df.info())        # structure and missing data
print(df.describe())    # numeric summary

# Clean column names (remove spaces, lowercase, etc.)
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")

"""## Check for Duplicates

The purpose of this section is to make sure that there are no data or section that are duplicated. Duplicated data can cause the analysis to have errors and invalid conclusions
"""

import pandas as pd

# Load the CSV file
df = pd.read_csv('SaaS-Sales.csv')

#  Clean column names: remove leading/trailing spaces, lowercase, replace spaces with underscores
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")

# Count and print number of duplicates
num_duplicates = df.duplicated().sum()
print(f"Number of duplicate rows: {num_duplicates}")

# Drop duplicates and confirm the new shape
df = df.drop_duplicates()
print(f"DataFrame shape after dropping duplicates: {df.shape}")

"""## Fix date columns

The column names in the data need to standardized, such as lowercase, replace spaces with "_". Converts order_date to datetime so it can be used in time-based operations and visualizations. Converting order_date to datetime to so that it can be used for time based operations.
"""

import pandas as pd

# Load the data
df = pd.read_csv('SaaS-Sales.csv')

# Clean column names
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")

# Fix 'order_date' column if it exists
if 'order_date' in df.columns:
    # Convert to datetime (invalid entries become NaT)
    df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')

    # Extract year and month
    df['order_year'] = df['order_date'].dt.year
    df['order_month'] = df['order_date'].dt.month
    df['order_day'] = df['order_date'].dt.day
    df['order_weekday'] = df['order_date'].dt.day_name()

"""Remove outliers

This function removes extreme profit values from the data using IQR method. Extremely high and low profit values can make the business seem more profitable or gain more profit than it's actual profit.
"""

# Remove outliers
if 'profit' in df.columns:
    Q1 = df['profit'].quantile(0.25)
    Q3 = df['profit'].quantile(0.75)
    IQR = Q3 - Q1
    df = df[(df['profit'] >= Q1 - 1.5 * IQR) & (df['profit'] <= Q3 + 1.5 * IQR)]

"""## Visualization of Data

Visualizing Outliers
"""

# Set visual style
sns.set(style="whitegrid")

# Plot boxplots for 'sales' and 'profit'
for col in ['sales', 'profit']:
    if col in df.columns:
        plt.figure(figsize=(8, 4))
        sns.boxplot(x=df[col])
        plt.title(f'Boxplot of {col.title()}')
        plt.xlabel(col.title())
        plt.show()

"""**Boxplot Graph Explanation**:

These boxplots show how Sales and Profit are spread out in the SaaS-Sales data. Most of the values are low as you can see the range goes from -20 to around 40 profit. The median is near the bottom of the box, which shows that most of the numbers are on the lower side. Additionally, there are many dots outside the main box, meaning there are several unusual or rare cases with very high or very low values.

Histogram visualization to see skew and distribution
"""

for col in ['sales', 'profit']:
    if col in df.columns:
        plt.figure(figsize=(8, 4))
        sns.histplot(df[col], kde=True, bins=30)
        plt.title(f'Distribution of {col.title()}')
        plt.xlabel(col.title())
        plt.ylabel("Frequency")
        plt.show()

"""**Histogram Charts Explanation:**


---


**Distribution of Sales:**




The sales distribution is heavily right-skewed or gathered at the lower amount, meaning that most sales transactions are low in value, meanwhile a small number of transactions have much higher values. The majority of the sales are below 500, with frequency dropping while the sales amount increases. This chart shows that high amount of sales are rare and most revenue likely comes from many small transactions



**Distribution of Profit:**

The profit distribution is moderately right-skewed and more balanced compared to sales. Most profits are clustered around 0 to 10, with fewer cases of extreme losses or very high profits. There’s a visible tail on the right side, indicating some transactions bring in much higher profit, but they occur less often. This pattern reflects a typical business scenario where most deals yield modest profit, with a few highly profitable outliers.

Highlighting Outliers using IQR Thresholds
"""

def plot_outliers_iqr(col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    plt.figure(figsize=(8, 4))
    sns.histplot(df[col], kde=True, bins=30)
    plt.axvline(lower, color='red', linestyle='--', label='Lower Bound')
    plt.axvline(upper, color='red', linestyle='--', label='Upper Bound')
    plt.title(f'{col.title()} with Outlier Thresholds')
    plt.xlabel(col.title())
    plt.legend()
    plt.show()

# Apply to 'sales' and 'profit'
for col in ['sales', 'profit']:
    if col in df.columns:
        plot_outliers_iqr(col)

"""## Final Cleaning

After cleaning the whole data through previous stages, it is then converted into a cleaned file. The new file then will be used for deeper analysis, which is the tableau.
"""

df.info()
df.head()

df.to_csv("SaaS-Sales-CLEANED.xlsx", index=False)

"""# Data Analysis and Visualisation"""

# Import necessary libraries (based on your learned materials) - THIS CODE IS JUST TO LOAD FOR THE FUNCTIONS BELOW
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load CSV file correctly
df = pd.read_csv('SaaS-Sales.csv')

# Clean column names
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")

# Preview dataset
print(df.info())
print(df.head())

"""## Products Analysis and Understanding

### Product Level Summary
"""

# Group by product and summarize key metrics
product_summary = df.groupby('product').agg({
    'sales': 'sum',
    'profit': 'sum',
    'quantity': 'sum',
    'order_id': 'nunique'
}).reset_index()

# Add average metrics
product_summary['avg_price'] = product_summary['sales'] / product_summary['quantity']
product_summary['avg_profit_per_order'] = product_summary['profit'] / product_summary['order_id']

# Sort and preview
product_summary.sort_values(by='sales', ascending=False).head()

"""### Top Products by Sales and Quantity"""

top_sales = product_summary.sort_values(by='sales', ascending=False).head(10)
top_quantity = product_summary.sort_values(by='quantity', ascending=False).head(10)

print("Top 10 Products by Sales:")
print(top_sales[['product', 'sales']])

print("\nTop 10 Products by Quantity Sold:")
print(top_quantity[['product', 'quantity']])

"""### Outlier Detection"""

import pandas as pd

# Load the dataset
df = pd.read_csv('SaaS-Sales.csv')

# Clean column names
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")

# Group by product and aggregate profit
product_summary = df.groupby('product').agg({
    'profit': 'sum'
}).reset_index()

# Rename for clarity
product_summary = product_summary.rename(columns={'profit': 'profit_sum'})

# IQR method for detecting outliers in profit
Q1 = product_summary['profit_sum'].quantile(0.25)
Q3 = product_summary['profit_sum'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR

# Filter outliers
profit_outliers = product_summary[
    (product_summary['profit_sum'] < lower) | (product_summary['profit_sum'] > upper)
]

# Output the result
print("Outlier Products (Profit-based):")
print(profit_outliers[['product', 'profit_sum']])

"""Top 10 Products by Sales"""

plt.figure(figsize=(10, 5))
sns.barplot(data=top_sales, x='sales', y='product', palette='Blues_r')
plt.title('Top 10 Products by Sales')
plt.xlabel('Total Sales')
plt.ylabel('Product')
plt.tight_layout()
plt.show()

"""Top 10 Products by Quantity"""

plt.figure(figsize=(10, 5))
sns.barplot(data=top_quantity, x='quantity', y='product', palette='Greens_r')
plt.title('Top 10 Products by Quantity Sold')
plt.xlabel('Quantity')
plt.ylabel('Product')
plt.tight_layout()
plt.show()

"""Profit Distribution with Outlier Boundaries"""

plt.figure(figsize=(8, 4))
sns.boxplot(x=product_summary['profit'], color='orange')
plt.axvline(lower, color='red', linestyle='--', label='Lower Outlier Threshold')
plt.axvline(upper, color='red', linestyle='--', label='Upper Outlier Threshold')
plt.title('Boxplot of Product Profit')
plt.xlabel('Profit')
plt.legend()
plt.tight_layout()
plt.show()

"""## Customers Analysis and Understanding

### Customer Summary Table
"""

# Group by customer to summarize sales, profit, and order activity
customer_summary = df.groupby('customer').agg({
    'sales': 'sum',
    'profit': 'sum',
    'quantity': 'sum',
    'order_id': 'nunique'
}).reset_index()

# Rename columns for clarity
customer_summary = customer_summary.rename(columns={'order_id': 'num_orders'})

# Add average order value and profit
customer_summary['avg_order_value'] = customer_summary['sales'] / customer_summary['num_orders']
customer_summary['avg_profit_per_order'] = customer_summary['profit'] / customer_summary['num_orders']

# Preview
customer_summary.sort_values(by='sales', ascending=False).head()

"""### Segment Level Analysis"""

# Sales and profit by customer segment
segment_summary = df.groupby('segment').agg({
    'sales': 'sum',
    'profit': 'sum'
}).sort_values(by='sales', ascending=False)

print(segment_summary)

"""### Industry Level Analysis"""

# Sales and profit by industry
industry_summary = df.groupby('industry').agg({
    'sales': 'sum',
    'profit': 'sum'
}).sort_values(by='profit', ascending=False)

print(industry_summary)

"""## Profits from Time Analysis

### Monthly Analysis
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('SaaS-Sales.csv')

# Clean column names
df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')

# Convert order_date to datetime
df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')

# Drop rows where order_date is missing
df = df.dropna(subset=['order_date'])

# Extract month and year
df['order_month'] = df['order_date'].dt.month
df['order_year'] = df['order_date'].dt.year

# Combine for grouping
df['year_month'] = df['order_date'].dt.to_period('M').astype(str)  # e.g., "2024-03"

monthly_summary = df.groupby('year_month').agg({
    'sales': 'sum',
    'profit': 'sum',
    'quantity': 'sum'
}).reset_index()

plt.figure(figsize=(14, 6))

# Line plot of total sales per month
sns.lineplot(data=monthly_summary, x='year_month', y='sales', marker='o', label='Sales')
sns.lineplot(data=monthly_summary, x='year_month', y='profit', marker='s', label='Profit')

plt.title('📅 Monthly Trend: Sales and Profit Over Time')
plt.xlabel('Month (Year-Month)')
plt.ylabel('Amount')
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.legend()
plt.show()

# Group by year and month number
monthly = df.groupby(['order_year', 'order_month'])[['sales', 'profit']].sum().reset_index()

# Month name (optional)
monthly['month_name'] = pd.to_datetime(monthly['order_month'], format='%m').dt.strftime('%b')

plt.figure(figsize=(16, 6))
sns.barplot(data=monthly, x='month_name', y='sales', hue='order_year')
plt.title('Monthly Sales Across Years')
plt.xlabel('Month')
plt.ylabel('Total Sales')
plt.grid(True)
plt.tight_layout()
plt.show()

"""Aggregate Monthly Sales and Profit"""

monthly_trend = df.groupby('year_month').agg({
    'sales': 'sum',
    'profit': 'sum'
}).reset_index()

# Convert period to timestamp for plotting
monthly_trend['year_month'] = monthly_trend['year_month'].dt.to_timestamp()

print(monthly_trend.head())

"""Visualize Sales and Profit by Time"""

plt.figure(figsize=(12, 6))

# Sales Trend
sns.lineplot(data=monthly_trend, x='year_month', y='sales', label='Sales', marker='o')

# Profit Trend
sns.lineplot(data=monthly_trend, x='year_month', y='profit', label='Profit', marker='s')

plt.title("Monthly Sales and Profit Trend")
plt.xlabel("Time (Year-Month)")
plt.ylabel("Amount")
plt.xticks(rotation=45)
plt.legend()
plt.tight_layout()
plt.grid(True)
plt.show()

"""### Time Trend Analysis by Week

Load Data
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Set style for better visuals
sns.set(style="whitegrid")

# Load data
df = pd.read_csv('SaaS-Sales.csv')

# Clean column names
df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')

# Convert to datetime
df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')

# Remove rows with invalid dates
df = df.dropna(subset=['order_date'])

# Extract components
df['year'] = df['order_date'].dt.year
df['month'] = df['order_date'].dt.month_name()
df['week_of_month'] = df['order_date'].apply(lambda d: (d.day - 1) // 7 + 1)  # Week 1–5

"""Aggregate data"""

# Group and aggregate
weekly_trend = df.groupby(['year', 'month', 'week_of_month']).agg({
    'sales': 'sum',
    'profit': 'sum'
}).reset_index()

# Sort for better plotting
weekly_trend = weekly_trend.sort_values(by=['year', 'month', 'week_of_month'])

"""Visualize Weekly Sales Trend"""

plt.figure(figsize=(14, 6))
sns.lineplot(data=weekly_trend, x='week_of_month', y='sales', hue='month')
plt.title('📊 Weekly Sales Trend by Month')
plt.xlabel('Week of Month')
plt.ylabel('Sales')
plt.legend(title='Month')
plt.xticks([1, 2, 3, 4, 5])
plt.tight_layout()
plt.show()

"""##Time Trend Sales Analysis by Day of the Week"""

# Define correct weekday order
weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

# Sort based on this order
weekday_summary['order_day'] = pd.Categorical(weekday_summary['order_day'], categories=weekday_order, ordered=True)
weekday_summary = weekday_summary.sort_values('order_day')

# Plot sales per weekday
sns.barplot(data=weekday_summary, x='order_day', y='sales', palette='Blues_d')
plt.title('📆 Total Sales by Day of the Week')
plt.xlabel('Day of the Week')
plt.ylabel('Total Sales')
plt.grid(True)
plt.tight_layout()
plt.show()

# Profit plot
plt.figure(figsize=(12, 6))
sns.barplot(data=weekday_summary, x='order_day', y='profit', palette='Greens_d')
plt.title('💰 Total Profit by Day of the Week')
plt.xlabel('Day of the Week')
plt.ylabel('Total Profit')
plt.grid(True)
plt.tight_layout()
plt.show()

"""# Hypothesis Testing

## Normal Distribution Test

This normal distribution test is done to determine whether a dataset follows a normal distribution. Confirming normality helps ensure the validity of these tests to help the next step of the analysis.

Importing Data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import shapiro, normaltest, probplot

"""Load and Prepare the Data"""

# Load the CSV file
df = pd.read_csv('SaaS-Sales.csv')

# Clean column names
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")

# Display numeric columns only
numeric_cols = df.select_dtypes(include=np.number).columns
print(" Numeric columns:", list(numeric_cols))

"""### Shapiro-Wilk Test for Normality"""

for col in numeric_cols:
    data = df[col].dropna()
    stat, p = shapiro(data)
    print(f"\n🔎 Shapiro-Wilk Test for '{col}':")
    print(f"Test Statistic = {stat:.4f}, p-value = {p:.4f}")
    if p > 0.05:
        print("Likely normal (fail to reject H₀)")
    else:
        print("Not normal (reject H₀)")

"""**Explanation**:

From this calculation and the display, we can see that the data is not normally distributed.
- p > 0.05 in Shapiro-Wilk test means the data follows normal distribution.
- p ≤ 0.05 means the data does not follow normal distribution

None of the numeric data in the dataset is normally distributed because the p-value is 0.00.

### Visualize Distribution (Histogram + Q-Q Plot)

The purpose of visualizing the histogram and Q-Q plot is to see how this data distributes. To make sure and confirm if the data is normally distributed or not.
"""

for col in numeric_cols:
    plt.figure(figsize=(12, 5))

    # Histogram
    plt.subplot(1, 2, 1)
    sns.histplot(df[col].dropna(), kde=True)
    plt.title(f"Histogram: {col}")

    # Q-Q plot
    plt.subplot(1, 2, 2)
    probplot(df[col].dropna(), dist="norm", plot=plt)
    plt.title(f"Q-Q Plot: {col}")

    plt.tight_layout()
    plt.show()

"""Based on the histograms and Q-Q plots above, none of the variables in the dataset follow a normal distribution. The Q-Q plots mostly do not follow the red line of the data and many of them have an outlier at the end of the “tail”. As for the histograms they also show signs of abnormal distributions, the histograms also have outliers. In conclusion, the profits are not evenly or normally distributed.

### Pearson Correlation Tests

#### Pearson Correlation Test between time trend with profit and sales

Pearson correlation test is done to find the correlation between time trend and profit sales increase. This is to confirm whether they both have relation to one another. This is to test whether the hypothesis of relevance between time trend and profits are positive.
"""

# 1. Import Libraries
import pandas as pd
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
import seaborn as sns

# 2. Load the CSV file
df = pd.read_csv('SaaS-Sales.csv')

# 3. Clean column names
df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')

# 4. Convert order_date to datetime
df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')

# 5. Drop rows with missing dates or sales/profit
df = df.dropna(subset=['order_date', 'sales', 'profit'])

# 6. Create time trend variable (e.g., days since first order)
df['days_since_start'] = (df['order_date'] - df['order_date'].min()).dt.days

# 7. Pearson correlation test between time and sales/profit
sales_corr, sales_p = pearsonr(df['days_since_start'], df['sales'])
profit_corr, profit_p = pearsonr(df['days_since_start'], df['profit'])

# 8. Print results
print("Pearson Correlation Results")
print(f"Sales vs. Time: r = {sales_corr:.4f}, p = {sales_p:.4f}")
print(f"Profit vs. Time: r = {profit_corr:.4f}, p = {profit_p:.4f}")

# 9. Optional: visualize trend
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.scatterplot(data=df, x='days_since_start', y='sales')
plt.title("Sales over Time")

plt.subplot(1, 2, 2)
sns.scatterplot(data=df, x='days_since_start', y='profit')
plt.title("Profit over Time")

plt.tight_layout()
plt.show()

"""**Result Analysis**
The Pearson correlation results show that there is no significant relationship between time and either sales or profit. The correlation between sales and time is very weak and slightly negative (r = -0.0095, p = 0.3406), while the correlation between profit and time is also very weak and slightly positive (r = 0.0044, p = 0.6621). Since both p-values are much greater than 0.05, we cannot say there is a meaningful trend over time for either sales or profit. In simple terms, sales and profit did not consistently increase or decrease as time passed—they stayed roughly the same with random ups and downs.

#### Pearson Tests between top products and profits

Load Data
"""

import pandas as pd

# Load your dataset
df = pd.read_csv('SaaS-Sales.csv')
df.head()


df.info()
df.describe()

"""Identifying top selling products"""

top_products = df.groupby('Product')['Sales'].sum().sort_values(ascending=False).head(10)
print(top_products)

"""Aggregate for top profitting products"""

profit_by_product = df.groupby('Product')['Profit'].sum()

# Combine into a single DataFrame
top_df = pd.DataFrame({
    'Sales': top_products,
    'Profit': profit_by_product[top_products.index]
})
top_df

"""Perform pearson correlation test"""

from scipy.stats import pearsonr

r_value, p_value = pearsonr(top_df['Sales'], top_df['Profit'])
print(f'Pearson correlation: r = {r_value:.4f}, p = {p_value:.4f}')

import seaborn as sns
import matplotlib.pyplot as plt

sns.scatterplot(data=top_df, x='Sales', y='Profit')
plt.title('Correlation Between Sales and Profit (Top-Selling Products)')
plt.show()

"""The Pearson correlation test between sales and profit among the top-selling products shows a correlation coefficient of r = 0.0704 with a p-value = 0.8467. This indicates an  insignificant or weak ositive relationship between sales and profit. In simple terms, higher sales do not necessarily lead to higher profit for these top products, and the high p-value suggests that any observed correlation could easily be due to random chance.

# Conclusion and Recommendations

## Conclusion

The analysis of the SaaS-Sales dataset using Pearson correlation and visual inspection leads to several important conclusions:

1. The correlation between time and sales (r = -0.0095, p = 0.3406) and between time and profit (r = 0.0044, p = 0.6621) shows that neither of them have changed over time. This means the business doesn't have consistent growth or decline in sales or profit across the period that was observed.

2. No Correlation Between Sales and Profit:
Among the top selling products, the correlation between sales and profit is very weak and statistically insignificant (r = 0.0704, p = 0.8467). Which means selling more doesn’t guarantee better profitability. Some products may sell a lot but generate little or even negative profit, while others may have strong margins but low sales volume.

## Recommendations

Strategies:

These findings suggest that the current sales performance is not being driven by time-based factors (e.g., seasonal growth or consistent market expansion). Therefore, improvement strategies should focus on promoting through different strategies other than time based.

- Promoting products with high margin
- underperforming products
- Reviewing pricing cost
- discounting strategies for low-profit high-sales items

Exploring customer segments or regions with untapped potential
Using product bundling or strategic promotions to increase profitable conversions
"""